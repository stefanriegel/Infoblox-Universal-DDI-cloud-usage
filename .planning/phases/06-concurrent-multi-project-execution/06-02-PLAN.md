---
phase: 06-concurrent-multi-project-execution
plan: 02
type: execute
wave: 2
depends_on:
  - "06-01"
files_modified:
  - gcp_discovery/discover.py
  - main.py
autonomous: true
requirements:
  - EXEC-03
  - EXEC-04
  - EXEC-05

must_haves:
  truths:
    - "Running a multi-project scan iterates all enumerated projects concurrently via ThreadPoolExecutor"
    - "Each project worker creates a new GCPDiscovery with shared compute clients and its own dns.Client"
    - "Console prints [N/total] project-id with resource breakdown as each project completes"
    - "Every discovered resource has a project_id field and a resource_id prefixed with project_id"
    - "Failed projects are printed inline and summarized at the end of the scan"
    - "Aggregated totals across all projects are printed at end of scan"
  artifacts:
    - path: "gcp_discovery/discover.py"
      provides: "Concurrent multi-project discovery loop with progress output and failure handling"
      contains: "ThreadPoolExecutor"
    - path: "main.py"
      provides: "Forwarding of --workers flag to GCP discover.py"
      contains: "gcp_args.workers"
  key_links:
    - from: "gcp_discovery/discover.py"
      to: "gcp_discovery/gcp_discovery.py"
      via: "GCPDiscovery(config, shared_compute_clients=shared) constructor call inside worker"
      pattern: "shared_compute_clients"
    - from: "gcp_discovery/discover.py"
      to: "gcp_discovery/config.py"
      via: "get_gcp_credential() and enumerate_gcp_projects() called before worker pool"
      pattern: "get_gcp_credential|enumerate_gcp_projects"
    - from: "main.py"
      to: "gcp_discovery/discover.py"
      via: "gcp_args namespace forwarded to gcp_main()"
      pattern: "gcp_args.workers"
---

<objective>
Replace the Phase 5 single-project scan path in discover.py with a concurrent multi-project
worker loop that scans all enumerated projects in parallel, prints per-project progress with
resource breakdowns, annotates every resource with project_id, and prints a failed-project
summary at the end.

Purpose: This is the core Phase 6 deliverable — multi-project concurrent discovery modeled
directly after the Azure multi-subscription pattern in azure_discovery/discover.py.

Output: Modified `gcp_discovery/discover.py` with concurrent worker loop and `main.py` with
updated flag forwarding.
</objective>

<execution_context>
@/Users/sr/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sr/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-concurrent-multi-project-execution/06-RESEARCH.md
@.planning/phases/06-concurrent-multi-project-execution/06-01-SUMMARY.md
@gcp_discovery/discover.py
@gcp_discovery/gcp_discovery.py
@gcp_discovery/config.py
@azure_discovery/discover.py
@main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement concurrent multi-project discovery loop in discover.py</name>
  <files>gcp_discovery/discover.py</files>
  <action>
Rewrite the main body of `main()` in `gcp_discovery/discover.py` to replace the Phase 5
`projects[0]` single-project path with a concurrent multi-project worker loop. Model directly
on the Azure pattern in `azure_discovery/discover.py` lines 258-340.

**New imports at top of file:**
- `import threading`
- `import time`
- `from concurrent.futures import ThreadPoolExecutor, as_completed`

**Pre-worker setup (after `enumerate_gcp_projects()` call and banner print):**

1. Fetch regions once: `all_regions = get_all_gcp_regions()` (already exists, keep as-is)

2. Build shared compute clients ONCE before the worker pool:
```python
from google.cloud import compute_v1
shared_compute_clients = {
    "instances":        compute_v1.InstancesClient(credentials=credentials),
    "zones":            compute_v1.ZonesClient(credentials=credentials),
    "networks":         compute_v1.NetworksClient(credentials=credentials),
    "subnetworks":      compute_v1.SubnetworksClient(credentials=credentials),
    "addresses":        compute_v1.AddressesClient(credentials=credentials),
    "global_addresses": compute_v1.GlobalAddressesClient(credentials=credentials),
}
```

3. Initialize worker state:
```python
lock = threading.Lock()
errors = []
all_native_objects = []
scanned_projects = []
completed_count = 0
total = len(projects)
effective_workers = min(args.workers, total)  # silent cap per locked decision
scan_start = time.monotonic()
```

**Worker function `discover_project(project_info)`:**

Define as a closure inside `main()` (same pattern as Azure `discover_subscription`):

```python
def discover_project(project_info):
    """Scan one GCP project. Returns (project_id, resources, type_counts)."""
    project_id = project_info.project_id
    config = GCPConfig(
        project_id=project_id,
        regions=all_regions,
        output_directory="output",
        output_format=args.format,
    )
    discovery = GCPDiscovery(config, shared_compute_clients=shared_compute_clients)
    native_objects = discovery.discover_native_objects(max_workers=args.workers)

    # EXEC-04: annotate every resource with project_id
    # EXEC-05: prefix resource_id with project_id for uniqueness
    for r in native_objects:
        r["project_id"] = project_id
        r["resource_id"] = f"{project_id}:{r['resource_id']}"

    # Count resources by type for progress output
    type_counts = {}
    for r in native_objects:
        t = r.get("resource_type", "unknown")
        type_counts[t] = type_counts.get(t, 0) + 1

    return project_id, native_objects, type_counts
```

**Executor loop (modeled on Azure discover.py lines 296-313):**

```python
with ThreadPoolExecutor(max_workers=effective_workers) as executor:
    future_to_project = {
        executor.submit(discover_project, pi): pi for pi in projects
    }
    for future in as_completed(future_to_project):
        pi = future_to_project[future]
        project_id = pi.project_id
        try:
            result_pid, native_objects, type_counts = future.result()
            with lock:
                completed_count += 1
                all_native_objects.extend(native_objects)
                scanned_projects.append(result_pid)
                # EXEC-03: [N/total] project-id — resource breakdown
                breakdown_parts = []
                # Ordered resource types for consistent output
                for rtype in ("compute-instance", "vpc-network", "subnet",
                              "reserved-ip", "dns-zone", "dns-record"):
                    if rtype in type_counts:
                        breakdown_parts.append(f"{type_counts[rtype]} {rtype}")
                # Append any remaining types not in the ordered list
                for rtype, count in sorted(type_counts.items()):
                    if rtype not in ("compute-instance", "vpc-network", "subnet",
                                     "reserved-ip", "dns-zone", "dns-record"):
                        breakdown_parts.append(f"{count} {rtype}")
                suffix = " \u2014 " + ", ".join(breakdown_parts) if breakdown_parts else ""
                print(f"[{completed_count}/{total}] {result_pid}{suffix}")
        except Exception as e:
            with lock:
                completed_count += 1
                errors.append({"project_id": project_id, "error": str(e)})
                print(f"[{completed_count}/{total}] {project_id}: FAILED \u2014 {e}")
```

Note: `completed_count` is a nonlocal int modified under lock. Use `nonlocal completed_count` in the closure or track it as `completed_count` outside and reference via lock. Since the variable is in the outer `main()` scope, declare it before the loop and use it directly (Python closures capture by reference for mutable-via-rebinding, so put `nonlocal completed_count` in the loop body, OR use a list `[0]` trick). The simplest approach: keep it as a plain variable in `main()` scope and do NOT use `nonlocal` since the `for` loop is not inside a nested function — the executor loop runs directly in `main()` scope.

**Post-loop output:**

```python
elapsed = time.monotonic() - scan_start

# Aggregated summary
print(f"\nTotal resources found across all projects: {len(all_native_objects)}")

# Failed project summary (per locked decision)
if errors:
    print(f"\nFailed projects ({len(errors)}):")
    for err in errors:
        print(f"  {err['project_id']}: {err['error']}")

print(f"\nScan complete: {len(scanned_projects)}/{total} projects succeeded ({elapsed:.1f}s total)")
```

**Exit code (Claude's discretion):** Return `1` if `errors` is non-empty, `0` otherwise. This matches Azure behavior where any failed subscription = non-zero exit.

**Keep the existing post-scan logic** (count_results, save_unknown_resources, print_discovery_summary, licensing calculator, proof manifest) but update it to work with `all_native_objects` and `scanned_projects` from the worker loop instead of from a single `discovery` instance:

- Replace `discovery.count_resources()` with a new `ResourceCounter("gcp").count_resources(all_native_objects)` call. Import `ResourceCounter` from `shared.resource_counter`.
- Replace `discovery.save_discovery_results(...)` with `save_discovery_results(...)` from `shared.output_utils` (already imported for `save_unknown_resources`).
- The `scanned_projects` list for the proof manifest is now populated by the worker loop (not `[p.project_id for p in projects]`).

**Remove the old single-project code block** that creates a single `GCPConfig`, single `GCPDiscovery`, and single `discover_native_objects()` call. All of that is replaced by the worker loop above.

**Update the banner output** to show `Project workers: {effective_workers}` (not just `Parallel workers: {args.workers}`) so operators see the actual concurrency level.

**Default workers change:** Change the `--workers` default from `8` to `4` in the `argparse` section of `main()`. Per locked decision, GCP's project-level default should match Azure's `--subscription-workers` default of 4. When the standalone `gcp_discovery/discover.py` parser is used directly, it should default to 4.
  </action>
  <verify>
Run `python -c "from gcp_discovery.discover import main; print('import OK')"` to verify no syntax errors.

Grep for key patterns:
- `grep -n 'ThreadPoolExecutor' gcp_discovery/discover.py` — must find the worker pool
- `grep -n 'shared_compute_clients' gcp_discovery/discover.py` — must find client injection
- `grep -n 'project_id' gcp_discovery/discover.py` — must find resource annotation
- `grep -n 'FAILED' gcp_discovery/discover.py` — must find error handling
- `grep -n 'completed_count' gcp_discovery/discover.py` — must find progress counter
  </verify>
  <done>
discover.py main() creates shared compute clients, spawns a ThreadPoolExecutor that runs discover_project() per project, prints [N/total] progress with resource breakdown, annotates all resources with project_id, prefixes resource_id with project_id, prints aggregated summary, and prints failed-project summary. Default --workers changed to 4.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update main.py GCP argument forwarding</name>
  <files>main.py</files>
  <action>
The `main.py` file already forwards `gcp_args.workers` to GCP discover.py. Verify this forwarding is correct and no new flags are needed for Phase 6.

Check the GCP argument forwarding block (around line 316). It should already have:
```python
gcp_args.workers = args.workers
```

The `--workers` flag in main.py's GCP subcommand should match the new default of 4. Find the GCP argparse section in main.py where `--workers` is defined for the GCP subcommand and change the default from 8 to 4 if it exists.

If main.py has a top-level `--workers` that applies to both Azure and GCP, check that changing the default to 4 does not affect Azure. Azure has separate `--workers` (8, intra-subscription) and `--subscription-workers` (4, inter-subscription) — the GCP `--workers` flag is separate. Per the locked decision, `--workers` should be a shared flag. If main.py currently has the GCP `--workers` default at 8, change it to 4.

No new flags are needed for Phase 6 — the existing `--workers`, `--project`, `--org-id`, `--include-projects`, `--exclude-projects` flags already cover all Phase 6 functionality.
  </action>
  <verify>
Run `python main.py gcp --help 2>/dev/null | head -20` (or check manually) to verify the `--workers` flag shows the updated default.

Grep: `grep -n 'workers.*default' main.py` to verify the default value.
  </verify>
  <done>
main.py forwards the --workers flag to GCP discover.py. Default is 4 (matching Azure subscription-workers). No new flags needed for Phase 6.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from gcp_discovery.discover import main"` succeeds (no import errors)
2. `grep 'ThreadPoolExecutor' gcp_discovery/discover.py` shows the concurrent worker pool
3. `grep 'shared_compute_clients' gcp_discovery/discover.py` shows shared client construction
4. `grep 'project_id.*resource_id' gcp_discovery/discover.py` shows resource annotation
5. `grep 'FAILED' gcp_discovery/discover.py` shows per-project error handling
6. `grep 'completed_count.*total' gcp_discovery/discover.py` shows progress output
7. `grep 'Failed projects' gcp_discovery/discover.py` shows end-of-scan summary
8. The `--workers` default in both discover.py and main.py is 4
9. `all_native_objects` aggregates resources from ALL projects (not just first)
10. `scanned_projects` list is populated by successful workers (not static from enumeration)
</verification>

<success_criteria>
- Multi-project scan runs all projects concurrently with progress output (EXEC-03)
- Every resource has `project_id` field in its dict (EXEC-04)
- Every `resource_id` is prefixed with `project_id:` for uniqueness (EXEC-05)
- Failed projects skip-and-continue with end-of-scan summary (locked decision)
- Aggregated summary across all projects printed at end (locked decision)
- Default workers = 4 matching Azure subscription-workers (locked decision)
</success_criteria>

<output>
After completion, create `.planning/phases/06-concurrent-multi-project-execution/06-02-SUMMARY.md`
</output>
